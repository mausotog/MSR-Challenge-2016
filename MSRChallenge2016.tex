% This is "sig-alternate.tex" V2.1 April 2013
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate-05-2015}
\usepackage{graphicx}
\usepackage[usenames,dvipsnames]{color}
\usepackage{soul} 
\usepackage{booktabs}

\newcommand{\todo}[1]
  {{\scriptsize \textbf{\color{red} {#1}}}}


\graphicspath{ {} }


\begin{document}

% Copyright
\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\doi{10.475/123_4}

% ISBN
\isbn{123-4567-24-567/08/06}

%Conference
%\conferenceinfo{PLDI '13}{June 16--19, 2013, Seattle, WA, USA}

%\acmPrice{\$0.00}

%
% --- Author Metadata here ---
\conferenceinfo{MSR}{2016 Austin, Texas USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{A deeper look into bug fixes: patterns, replacements, deletions, and additions}

%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{3} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Mauricio Soto\\
       \affaddr{School of Computer Science}\\
       \affaddr{Carnegie Mellon University}\\
       \affaddr{Pittsburgh, PA}\\
       \email{mauriciosoto@cmu.edu}
       % 3rd. author
\alignauthor Ferdian Thung\\
       \affaddr{University}\\
       \affaddr{City}\\
       \email{email@domain.com}
% 2nd. author
\alignauthor Chu Pan Wong   \\
       \affaddr{Carnegie Mellon University}\\
       \affaddr{Pittsburgh, PA}\\
       \email{email@domain.com}       
\and  % use '\and' if you need 'another row' of author names
% 4th. author
\alignauthor
Claire Le Goues\\
       \affaddr{School of Computer Science}\\
       \affaddr{Carnegie Mellon University}\\
       \affaddr{Pittsburgh, PA}\\
       \email{clegoues@cs.cmu.edu}
% 5th. author
\alignauthor
David Lo\\
       \affaddr{University}\\
       \affaddr{City}\\
       \email{author@institution.edu}       
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
%\additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
%email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
%(The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
\date{February 2, 2016}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
\textbf{
With the rise of automated program repair in the last couple of years, there are a lot of questions to remain unanswered. We performed a study on the Github dump of September 2015 in which we analyzed all the fixing revisions of this dataset, and found all the replacements made in these 46,301,429 files from real life projects in Github.
}

\textbf{
We found very valuable information to guide automatic software repair, such as the most common and least common replacements made by human developers in order to build a successful patch; we analyzed the most and least common statement to replace others, and the most and least likely statement to get replaced by other. We also analyzed, once you have a statement that you want to replace (a faulty statement), what are the most likely statements to replace it for. These information will help automatic program repair tools to be more maintainable and user friendly, taking it one step closer to what humans developers do to repair their code.
}

\end{abstract}


%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>  
\end{CCSXML}


%
% End generated code
%

%
%  Use this command to print the description
%
\printccsdesc

% We no longer use \terms command
%\terms{Theory}

\keywords{Automatic error repair; Maintainability; Human-like patches}

\section{Introduction}

Automatic bug repair is the branch of computer science that deals with automated
ways to repair errors in software. There have been several approaches taken
towards improving the different methodologies to repair bugs in software
automatically~\cite{kim2013,weimer2009,legoues2012,pan2009}, one of the
most accepted approaches so far has been GenProg~\cite{weimer2009,legoues2012}, an
evolutionary program repair tool which applies four different kinds of possible
edits to code in order to find a patch for a given bug. These changes are the
following: Delete, Replace, Swap and Append.

\todo{We need to integrate the PAR and operator discussion a bit better.  Let's
  take it up a level of abstraction: there is initial work in applying these
  techniques to Java that propose a number of mutation operators, and we study
  the degree to which they apply in practice and make recommendations based on
  human-provided repairs.  Doing so is a good idea, because patches inspired by
  human-created repairs are empirically demonstrated to be rated more highly by
  developers (cite PAR).}
\todo{I also thought we were doing Par first, and then the replacement studies?}

Another well known approach is PAR~\cite{kim2013}, which creates 10 different
repair templates and applies them to the buggy code in an effort to repair
it. In this paper we have taken 7 out of the 10 PAR templates and tested how
common they are in the repairs made by programmers in the latest official data
dump of Github as of September 2015 provided by~\cite{dyer2013}.

Our research aims to provide guidelines to improve three out of four of these
possible changes: Delete, Replace and Append. It will provide the data necessary
for this tool and other approaches to have a guide on what is the way in which
human programmers change their code when coding a fix for a bug. This way it
will make it more likely for automatic error repair approaches to succeed in
finding a patch for a particular error, and also, to provide automatic error
repair software with heuristics to make the patches more human-like and
therefore more readable and maintainable by human developers.

In this article we study the frequency with which human programmers replace,
delete and append different statements to their source code in order to fix a
bug.

\section{Related Work}
A similar study was performed by Dongsun Kim et al.~\cite{kim2013} in which they
look for the most common ways in which programmers patch bugs in software. The
researchers developed a variation of the tool Genprog~\cite{weimer2009,legoues2012}
with several different templates resembling patters programmers use to patch
bugs.

A good complement for this paper is~\cite{zhong2015}, where they search additions and
deletions in a smaller data set. In this paper, we focus on the replacements,
which have been left out of the analysis of~\cite{zhong2015}.

\section{Methodology}
Our methodology is applied to the latest official data dump of Github as of
September 2015 provided by~\cite{dyer2013}. In the following subsections, we first
describe data source and objective of our study. We then describe how we detect
bug fixing patterns in PAR to find out how common they are the dataset. Last but
not least, we describe our approach to detect Delete, Replace, and Append
bug-fixing patterns.

\begin{table*}
	\centering
	\caption{Amount of appearances per replace kind}
	\resizebox{\textwidth}{100pt}{
		\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|} \hline
			&Assert&Break&Continue&Do& For&If&Label&Return&Case&Switch& Synchronized&Throw&Try&TypeDecl&While
			\\ \hline
			\texttt{Assert}	&-&340&	232	&27	&517	&1764&	14&	1260	&211	&191&	82&	1004	&352	&0	&206
			\\ \hline
			\texttt{Break} &455	&-&	1254&	176	&3757	&9096&	90&	10538&	771	&455&	672	&3123&	3660&	18&	2278
			\\ \hline
			\texttt{Continue} & 229	&1384	&-&	221	&1909	&3094&	85&	3486	&772&	635	&233&	1227&	1493&	30&	1086
			\\   \hline
			\texttt{Do} &32	&205&	171	&-&	474	&648	&12&	543	&106	&92&	44	&227	&325&	4&	558
			\\ \hline
			\texttt{For} &505	&3366	&1516	&316	&-&	14547	&56	&14924	&2791	&1852	&1044&	5642&	5796	&41	&6582
			\\ \hline
			\texttt{If}& 1402&	8820&	2430&	476	&13703&-&		247	&30489	&8639	&5710	&2472&	8960&	13714&	84&	5622
			\\ \hline
			\texttt{Label} &19&	44	&46	&6&	56	&255	&-&	270	&44	&22	&5	&80&	53&	2	&33
			\\ \hline
			\texttt{Return} &1219&	8603	&3009	&531	&12620	&28536&	164	&-&	6088&	3657&	2021&	15346&	11486&	71	&4684
			\\ \hline
			\texttt{Case} &298	&839&	506	&125&	3002&	8039&	33&	6261	&-&	126	&649&	2206&	2784&	24&	1864
			\\ \hline
			\texttt{Switch}&281	&533&	379	&56	&1997	&5175&	22	&4090&	126	&-&	212	&1571	&1311	&7&	878
			\\ \hline
			\texttt{Synchronized} &79&	606&	289&	66	&1122&	3070&	158&	2853	&562&	256	&-&	1303&	1274&	8	&466
			\\ \hline
			\texttt{Throw} &821&	2844&	1195	&197&	5993	&10664&	113&	16688&	2002	&1446&	936		&-&4443&	37&	1934
			\\ \hline
			\texttt{Try}& 440&	3976&	1254&	214	&6414&	16294&	45	&14119	&3178	&1509	&1243&	4795	&-&	46&	2218
			\\ \hline
			\texttt{TypeDecl} &2&	35&	15	&1&	49	&83&	1&	137	&21&	9&	4&	35&	46	&-&	11
			\\ \hline
			\texttt{While}& 301&	2295&	1016&	937	&8257&	6587&	41	&6280&	1866&	766	&606&	2028&	2677&	27	&-
			\\ \hline
		\end{tabular}
	}
\end{table*}

\subsection{Data Source and Research Objective}
We used the Boa platform~\cite{dyer2013} to be able to query the Github repository
looking for patterns that humans use to patch errors in the their code. Our main
goal is to provide guidelines for automatic error fixing approaches by providing
necessary information regarding historical replace, delete and append statements
in human written patches. 

\subsection{Detecting PAR Bug Fixing Pattern}\label{sec:method}
The bug fixing patterns that we analyze in this work are adopted from Kim et
al. paper~\cite{kim2013}. The following is the description of each considered
pattern. 
\todo{Ferdian: why are we only studying 7?  It's OK to not study all of them,
  but we need to explain why.}
  \begin{enumerate}
	\item {\bf Altering method parameters (AMP)}\\
	{\bf Example:} obj.method(v1,v2) $\rightarrow$ obj.method(v1,v3)\\
	{\bf Description:} This pattern changes the input for method parameters.
	\item {\bf Calling another method with the same parameters (MSM)}\\
	{\bf Example:} obj.method1(param) $\rightarrow$ obj.method2(param)\\
	{\bf Description:} This pattern changes the method name.
	\item {\bf Calling another overloaded method with one more parameter (COM)}\\
	{\bf Example:} obj.method(v1) $\rightarrow$ obj.method(v1,v2)\\
	{\bf Description:} This pattern adds one more parameter to the method.
	\item {\bf Changing a branch condition (CBC)}\\
	{\bf Example:} if(a != b) $\rightarrow$ if(a != b \&\& c == 0)\\
	{\bf Description:} This pattern adds or removes condition.
	\item {\bf Initializing an object (IAO)}\\
	{\bf Example:} Type obj $\rightarrow$ Type obj = new Type()\\
	{\bf Description:} This pattern adds a initialization to object declaration.
	\item {\bf Adding a null checker (ANC)}\\
	{\bf Example:} obj.m1() $\rightarrow$ if(obj!=null){obj.m1()}\\
	{\bf Description:} This pattern inserts a condition to check whether an object is null or not in order to prevent unexpected state.
	\item {\bf Adding an array out of bound checker (AOB)}\\
	{\bf Example:} arr[idx]=0 $\rightarrow$ if(idx<arr.length){arr[idx]=0}\\
	{\bf Description:} This pattern inserts a condition to check that an array index is within bound right before the index is used.
\end{enumerate}

To analyze bug fixing patterns, we utilizes Boa's language
infrastructure~\cite{dyer2013}. It contains a large number of dataset and provide
a domain specific language to perform analysis on source code. Despite this
advantage, Boa still have limited capability that prevents us from performing
precise detection of bug fixing patterns. For example, it is unable to perform a
diff between two file versions, which is needed to compare the pre-fix and the
post-fix version of a buggy file. Thus, rather than finding the exact count of
bug fixing patterns, we will find an approximate count of those patterns. The
following are descriptions on how we detect each pattern.

\begin{enumerate}
\item {\bf AMP Pattern}\\
  To detect AMP pattern, for both pre-fix and post-fix version of a buggy file,
  we create a custom method call signature. This signature contains method name,
  literal parameter, and variable parameter. we consider parameter containing
  custom expression as OTHER. This is due to inexistence of function that can
  print Abstract Syntax Tree (AST) back to source code. we discard method
  signatures that appear both in pre-fix and post-fix version. We then find
  whether there exists method with the same name and the same number of
  parameters, but different parameter signatures in pre-fix and post-fix
  version. If it exists, we consider that AMP pattern is found.
	
\item {\bf MSM Pattern}\\
  To detect MSM pattern, we create method signature similar like when detecting
  AMP pattern. This time, we find whether there exists method with the exact
  same parameter signature but different name in pre-fix and post-fix
  version. If it exists, we consider that MSM pattern is found.
	
\item {\bf COM Pattern}\\
  To detect COM pattern, we create method signature similar like when detecting
  AMP and COM pattern. This time, we find whether there are exists method with
  the same name, but the number of parameters in pre-fix and post-fix version
  differs by one. If it exists, we consider that COM pattern is found.
	
\item {\bf CBC Pattern}\\
  To detect CBC pattern, we count the number of { \em logical and} and {\em
    logical or} inside if conditional expression for both pre-fix and post-fix
  version. we assume that the addition/removal of logical operators indicate
  addition/removal of condition. If there are count differences between pre-fix
  and post-fix version, we consider that CBC pattern is found.
	
\item {\bf IAO Pattern}\\
  To detect IAO pattern, we count the number of NEW expression in variable
  declaration for both pre-fix and post-fix version. If there are count
  differences between pre-fix and post-fix version, we consider that IAO pattern
  is found.
	
\item {\bf ANC Pattern}\\
  To detect ANC pattern, we count the number of if conditional expression that
  contains {\em !=null} or {\em ==null} for both pre-fix and post-fix
  version. If there are count differences between pre-fix and post-fix version,
  we consider that ANC pattern is found.
	
\item {\bf AOB Pattern}\\
  To detect AOB pattern, we count the number of if conditional expression that
  contains {\em expr<var.length} or {\em var.length>expr} for both pre-fix and
  post-fix version. If there are count differences between pre-fix and post-fix
  version, we consider that AOB pattern is found.
	
\end{enumerate}
\todo{IMPORTANT: Ferdian, where did the results go?  Please add the results of
  the Par study into results somewhere where it fits.}

\subsection{Replace Query}
The replace query is a query in which first we set the project that we are going
to use, which is the Github dump of September 2015 provided by~\cite{dyer2013}.

We then create a visitor to traverse all the nodes of this repository. First
thing the visitor does is to ask weather the node is a Revision, and if it is,
then it asks if it is a fixing revision. Therefore we are analyzing only files
that were already committed and are now being reviewed to fix an error. We use
Boa's infrastructure to do this by visiting the node they label as "Revision"
and then calling the function defined by Boa "isfixingrevision()".

A replacement needs two statements, one that is deleted and another one that is
inserted.  We then go through all the files that have been changed in this data
set asking for two conditions: it must be a fixing revision and there must have
been a previous version of this file before. If these conditions are met then we
initialize two counters: one for the first statement we are analyzing (the one
that was deleted), and another one for the second statement we are analyzing
(the one that was inserted).

At this point we visit each statement in the previous state of that changed
file, before the change was applied and we count the amount of appearances of
the first statement we are analyzing (the one that was deleted), and then we
count the amount of appearances of the second statement we are analyzing (the
one that was inserted). We save these values, and do the same for the latest
version of the file and save these values as well.

We then compare these results to see if either the first and second statements
being analyzed increased or decreased. Depending on these we have two
conditionals:

If the amount of occurrences of the first statement decreased and the amount of
occurrences of the second statement increased on the same file, then we say that
the first statement was replaced by the second statement in that file. Likewise,
if the amount of occurrences of the first statement increased and the amount of
occurrences of the second statement decreased on the same file, we say that the
second statement was replaced by the first statement in that file. We count the
amount of files in which this happens for each of these two cases.

We need to test this with all the possible combinations of two different
statement types since we want to know what is the probability of an specific
statement to be replaced by another specific statement. For example, we want to
know how often does a For loop gets replaced by a While loop in a successful
human written patch, or how often does a Break statement gets replaced by a
Continue statement in a successful human written patch.

Because of this we have created Table 1 which records the amount of replaces happening from one kind of statement to another. It is a matrix of fifteen by fifteen slots that describes which statements are being replaced by which other. 

The numbers in the slots represent the amount of files in which that replacement took place. In the first column we have the list of statements that are being replaced, and on the top row we have the list of statements for which that statement was replaced for. For example, if we take the intersection of row 5 (DoStatement row) and column 2 (AsserStatement column), we have a 32, which means that from the 46,301,429 files categorized by Boa as fixing revisions in Github in the month of September 2015, only 32 of them replaced a DoStatement for an AssertStatement.

It is important to notice  that this analysis doesn't consider replacing an statement kind for a different version of the statement kind, since it is counting the amount of appearances of each statement kind. So, if for example, the developer modifies the condition inside of an if statement, the amount of if statements in the file is going to remain the same

\subsection{Delete/Append Query}
The Delete/Append Query is similar to the Replace Query with some variations to
count for the amount of times that a file was changed and a particular statement
kind was ei- ther deleted or append, and the rest of the statement kinds
remained the same.

We created a Boa file in which first we create an output variable to count all
the amount of files in which a statement kind was deleted or appended. Similar
to the previous query, we visit all the files labeled as fixing revisions, then
visit ev- ery node of the previous version of the revision, counting the amount
of statement kinds of each one of the possible fifteen different ones. We then
do the same for the later version of the revision, counting the different
statement kinds.

Finally, we compare the amounts of the previous version, with the new
version. For each of the revisions, we ask if the amount of statements stays the
same for all the statement kinds, except for X kind of statement. When the
amount of statements is different in the previous version than the latest
version for X kind of statement. Then we ask if X kinds of statement is greater
in the previous version or the latest ver- sion. If the amount of appearances of
X kind of statement is greater in the previous version than the latest version,
and all the remaining fourteen statement kinds have the same amount in the
previous than the latest version, then we say that one or more of the statements
of kind X were deleted. Otherwise, if the latest version has a greater amount of
ap- pearances of X kind of statement and the rest have the same amount of
appearances in both the previous and the latest version, then we say that one or
more statements of kind X were appended or added to the fixing revision.



\section{Results}

\todo{What is the dataset?}

This section investigates how do developer fix bugs from three different
perspectives. To begin with, we show some characteristics of real-world bug
fixes.  Afterwards, we look deeper to find out what kinds of statements are
usually added, modified or deleted by human developers to fix bugs. Lastly, we
show how common PAR's fixing templates are in our dataset. 

\subsection{Characteristics}

This subsection shows a bird's-eye view of real-world bug fixes at
ultra-large-scale. We did our analysis on the \emph{2015 September/Github}
dataset on Boa. This dataset includes $7,830,023$ Java projects with
$23,229,406$ revisions. Among all the revisions, $4,590,679$ revisions are
identified by Boa as bug fixing revisions. Our analysis in this subsection
focuses on these bug fixing revisions. We hope that this population
is big enough to reflect the bug fixing practice.

To begin with, we are interested in:

\begin{quote}
	How many files are changed to fix a bug?
\end{quote}

To answer this question, we identified all the bug fixing revisions in our
dataset and count the number of changed files. In total, $52,052,571$ files got
changed. Thus, on average each bug fixing revision changes 11.3 files.  This
number is surprisingly high because most automatic program repair techniques
assume that bugs are local in most cases. There could be several reasons behind
this number. It could be the case that other software artifacts (e.g.
documentation) are updated after bug fixing. It could also be the case that bug
fixing commit also contains changes that are not related to fixing bug (e.g. new
features, refactoring). To better understand why so many files are changed, we
continue to ask:

\begin{quote}
	What are the file types of those changed files?
\end{quote}

In Boa, each changed file is represented by a \emph{ChangedFile} node, and it
has a field called \emph{FileKind}, describing the type of each file.
Table~\ref{tbl:fileType} shows the file types that Boa could identify. To answer
this question, we iterate all the changed files in each bug fixing revision and
count the total number for each file type. Results are shown in
Table~\ref{tbl:fileType}.

\begin{table}
\centering
  \begin{tabular}{| c | r | r |}
  \hline
  File Kind & Total Number & Average \\ \hline \hline
  BINARY & 752,945 & 0.16 \\ \hline
  SOURCE\_JAVA\_ERROR & 2,073,558 & 0.45 \\ \hline
  SOURCE\_JAVA\_JLS2 & 2,607,413 & 0.57 \\ \hline
  SOURCE\_JAVA\_JLS3 & 15,748,967 & 3.43 \\ \hline
  SOURCE\_JAVA\_JLS4 & 83,798 & 0.02 \\ \hline
  TEXT & 541,023 & 0.12 \\ \hline
  XML & 6,818,299 & 1.49 \\ \hline
  UNKNOWN & 23426568 & 5.10 \\ \hline
  \end{tabular}
  \caption{File Types of Changed Files}
  \label{tbl:fileType}
\end{table}

\emph{SOURCE\_JAVA\_ERROR} file kind represents files that Boa failed to parse.
\emph{JLS2} refers to Java files that are written in all versions of Java
languages up to and including J2SE 1.4. \emph{JLS3} refers to J2SE 1.5, and JLS4
refers to J2SE 1.6 and 1.7. Boa documentation does not explicitly explain what
is \emph{TEXT} and what is \emph{UNKNOWN}. Our assumption is that \emph{TEXT}
refers to files whose file name ends with ``.txt'', and \emph{UNKNOWN} refers to
all other kinds of files (e.g. .c file, .cpp files).

From Table~\ref{tbl:fileType}, we can see that \emph{TEXT} files and
\emph{BINARY} files are changed least frequently. This is not surprising because
\emph{TEXT} files are usually documentation files, and \emph{BINARY} files
should only be changed in rare case (e.g.  milestone, new release, dependency
update). If we consider all 4 kinds of Java files as one kind, then on average
each bug fixing revision changes $4.47$ Java files. But still, it does not
conform to our assumption of bug locality. \emph{XML} files in Java projects
usually represents build files, thus changing one build file after each bug
fixing is not surprising.  What surprises us is that \emph{UNKNOWN} files are
changed most frequently. Note that all the projects in our dataset are Java
projects, so it is reasonable to assume that these files are not source files
that written in other programming languages.  One possible explanation is that
they are test resources. However, due to the limitations of Boa, we can not look
into the content of these \emph{UNKNOWN} files. 

Since Boa does not distinguish Java files that related to functionality and Java
files that related to testing, it could be the case that among those $4.47$
changed Java files, some of them are related to testing, such as modifying the
test cases and creating new test classes. To better understand what changes are
usually made to Java source code files, we continue to ask:

\begin{quote}
	How often do developers introduce new classes, methods, fields, variables in
	bug fixing commits?
\end{quote}

This research question is interesting because most automatic program repair
approaches could not create new classes/methods/fields/variables to fix bugs.
Most approaches guess and synthesize the patches based on existing source code.

Table~\ref{tbl:new} shows our analysis results. This table shows that
introducing new classes and new variables are rare. This is good news for
automatic program repair approaches. On average, we still have $0.69$ new method
per bug fix. If we consider the possibility of creating new test method (i.e.
methods that annotated by @Test if JUnit is used), this number is not that
scary. However, what surprises us is that $1.32$ fields are created per bug fix.
This indicates that automatic program repair approaches should pay attention to
the state of the class when fixing Java programs.

\begin{table}
\centering
  \begin{tabular}{| c | r | r |}
  \hline
  Introducing & Total Number & Average \\ \hline \hline
  Class & 729201 & 0.16 \\ \hline
  Methods & 3186867 & 0.69 \\ \hline
  Fields & 6076646 & 1.32 \\ \hline
  Variables & 924259 & 0.2 \\ \hline
  \end{tabular}
  \caption{File Types of Changed Files}
  \label{tbl:new}
\end{table}

\todo{Is there room/call to include any element of Chu Pan's replacement
  analysis in here?}

\subsection{Frequency of PAR Bug Fixing Pattern}\label{sec:freqfixpattern}
After running the bug pattern detection approach on Boa's dataset, I obtained the result shown in Table~\ref{tab:freqpattern}. The result shows that AMC, MSM, COM, CBC, IAO, ANC, and AOB pattern is found on 1.95\%, 1.69\%, 0.58\%, 4.23\%, 2.82\%, 2.90\%, and 0.28\% of buggy files. We can see that the most common pattern among these patterns is CBC and the least common pattern is AOB. If we assume that these patterns never appear together in the dataset, these patterns cover 14.21\% of buggy files. Kim et al. mentioned that the patterns that they use cover almost 30\% of real patches~\cite{kim2013}. Although we only detect 7 out of 10 bug fixing patterns, it is hard to believe that the remaining three patterns cover 15\% of buggy files, especially 14.21\% is a number we get when we assume that the patterns that we detect never appear together. Moreover, the percentage of each pattern is based on imprecise approximation which is likely to represent an upper bound rather than a lower bound. This suggests that common bug fixing patterns might not be as common as it appeared to be.

\begin{table}[!htb]
	\caption{Frequency of Bug Fixing Patterns}\label{tab:freqpattern}
	\centering
	\begin{tabular}{lr} 
		\hline
		& \textbf{GitHub}\\
		\hline
		\#Buggy Files & 46,301,429 \\ 
		\#AMP Pattern & 901,083 \\
		\#MSM Pattern & 783,073 \\
		\#COM Pattern & 270,128 \\ 
		\#CBC Pattern & 1,959,377 \\  
		\#IAO Pattern & 1,308,006 \\  
		\#ANC Pattern & 1,340,561 \\  
		\#AOB Pattern & 128,016 \\  
		\hline
	\end{tabular}
\end{table}

\subsection{Data Analysis}
Our queries were ran on the dump of Github of September 2015. In these real life
projects we found 46,301,429 files being fixed. In these files, there is a total
of 517,281 statement kinds being replaced, and 511,198 replacing these
statements kinds.

We analyzed each of these files looking for statement kinds being either replaced by other kinds, deleted or appended to the file.

We found that the most common Replacer statement kind, meaning the statement
kind that most commonly replaces others is the \textbf{If Statement} with
101,366 appearances. The least common Replacer, meaning the statement kind which
is less likely to replace other statement kinds is the \textbf{Type Declaration
  Statement} with 447 appearances.

The most common Replacee, which is the most likely statement kind to be replaced
by another statement kind is the \textbf{Return Statement} with 111,938
appearances. The least common Replacee, which is the least common statement to
be replaced by another statement kind is the \textbf{Type Declaration Statement}
with 399 appearances.

\subsubsection{Likeliness of replacement}
This study is directly applicable to automatic software repair. Most common
tools for automatic software repair~\cite{kim2013,weimer2009,legoues2012}
first find a faulty statement using several different fault localization
techniques~\cite{fry2010}, and once they have that faulty statement, they apply
different kinds of edits to it, usually at random. Which makes it equally likely
to replace an statement for another without looking at the kinds of statements
being replaced.

With this study, we are able to make a more accurate guess, when having a known faulty statement, to which statement it is more likely to replace it for. By performing simple math with the data of Table 1, we can find the most likely replacements that will end up being a successful patch. 

For example, if we have a faulty statement that happens to be a For Statement. Then we can look up in Table 1 the For Statement Column (6th column). If we add all the values in that column, we get 59,870 changes in which a For Statement was replaced by some other kind of statement. Then we calculate the percentage of each of the values in the column to see the likeliness of each of the statements to replace it for, and in this example we see that the top 3 most likely statements to replace a For Statement are an If statement with a 22.88\% likeliness, followed by a return statement with a 21.08\%, and third is a While statement with a 13.73\% of likeliness to replace a For Statement.


\subsubsection{Most common replacements}
Our results show that the most common replacement is when developers replace a Return statement for an If statement. This showed up in 30,489 files in the code database. The second most common replacement is an If statement being replaced by a Return statement, which had 28,536 appearances in the code database. Third to that, is a Return statement being replaced by a Throw statement.

\includegraphics[scale=0.5]{g1.png}

\subsubsection{Least common replacements}
On the opposite side of the data, we can see that the least common replacement was an Assert statement replacing a Type Declaration statement. This was the only replacement to have zero appearances in our search through the 46,301,429 files being fixed. Close to it, as the second least common replacement was replacing a Do statement for a Type Declaration statement, with one appearance in our code database, and on third place, the replacement of a Label being replaced by a Type Declaration Statement had one appearance in our code database.

\includegraphics[scale=0.5]{g2.png}





\section{Conclusions}
The findings of our study provide a set of useful guidelines for automatic
program repair tools with which their search for a patch can be now improved by
having the knowledge of how likely it is to replace a faulty statement by the
different kinds of statements available.
\todo{Expand to include summary of actual conclusions}


%\end{document}  % This is where a 'short' article might terminate


%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns


%\balancecolumns % GM June 2007
% That's all folks!
\end{document}
